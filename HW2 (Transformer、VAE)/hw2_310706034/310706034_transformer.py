# -*- coding: utf-8 -*-
"""2_1_code_2_0_(3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nHaNiL0tO-Zbest_V8Pt9q8ytUJ1Ggu8

# DL_HW2
"""

import torchtext
if torchtext.__version__ != '0.11.0':
  !pip install -U torchtext==0.11.0

print(torchtext.__version__)

import pandas as pd
import torch
import torchtext
from nltk.tokenize import word_tokenize
from torchtext.legacy.data import Field, LabelField, Iterator, BucketIterator, Example, Dataset

import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchtext.vocab import GloVe, FastText
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
import math
import numpy as np
import random
import nltk
import string
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from torch.optim.lr_scheduler import ReduceLROnPlateau

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

"""## Model Structure"""

class PositionalEncoding(nn.Module):
    def __init__(self, max_len, embedsz, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        
        self.pe = torch.zeros(max_len, embedsz).to(device)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div = torch.exp(torch.arange(0, embedsz, 2) * -(math.log(10000.) / d_model))
        
        self.pe[:, 0::2] = torch.sin(pos * div)
        self.pe[:, 1::2] = torch.cos(pos * div)
        
        self.dropout = nn.Dropout(dropout)
    
    
    def forward(self, x):
        out = x + self.pe[:x.size(0), :]
        return self.dropout(out)
        

collection = []
class Transformer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, max_len, d_model, num_layers, head, num_class, embedding_weight, dropout=0.2):
        super(Transformer, self).__init__()
        
        self.embedding_dim = embedding_dim
        self.max_len = max_len
        self.d_model = d_model
        self.head = head
        # self.dim_feedforward = num_layers
        self.dropout = dropout
        
        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)
        self.embedding_layer.weight.data.copy_(embedding_weight)
        self.embedding_layer.weight.requires_grad = False
        
        self.position_encoder = PositionalEncoding(max_len, embedding_dim)
        
        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, head, num_layers, dropout=0.2, activation='gelu')        
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers, norm=None)
        self.multiHeadAttention = nn.MultiheadAttention(embedding_dim,head)
        self.fc = nn.Linear(embedding_dim, d_model)
        self.tanh = nn.Tanh() #Tanh
        self.fc2 = nn.Linear(d_model, num_class)
    
    
    def generate_mask(self, batch_length):
        self.mask = torch.zeros(len(batch_length), self.max_len).to(device)
        for i, length in enumerate(batch_length):
            self.mask[i][:length] = 1
        masked = self.mask.float().masked_fill(self.mask == 0, float('-inf')).masked_fill(self.mask == 1, float(0.0))
        return masked
    
    
    def forward(self, x, masked, mode):
        embedding = self.embedding_layer(x) * math.sqrt(self.embedding_dim)
        out = self.position_encoder(embedding)
        out = out.permute(1, 0, 2)
        out= self.encoder(out, src_key_padding_mask=masked)
        # print(x.shape)
        # print(attn_output_weights.shape)
        if mode == 'test':
          attn_output,attn_output_weights= self.multiHeadAttention(out,out,out, masked)
          collection.append(attn_output_weights.detach().cpu())
        # # print(attn_output_weights[0:len(out)].detach().cpu())
        # sns.heatmap(out[0].detach().cpu())
        out = out.permute(1, 0, 2)
        self.mask = self.mask.unsqueeze(2)
        out = out.sum(1) / (self.mask.sum(1) + 1e-5)
        out = self.tanh(self.fc(out))
        out = self.fc2(out)
        
        return out

"""## Data Preprocessing"""

def dataframe_to_dataset(data, mode, idx_field, text_field, length_field, label_field):
    examples = []
    X = data['Text'].values.tolist()
    
    if mode == 'test':
        fields = [('id', idx_field), ('text', text_field), ('length', length_field)]
        for idx, text in enumerate(X):
            examples.append(Example.fromlist([idx, text, len(text)], fields))
    else:
        fields = [('id', idx_field), ('text', text_field), ('length', length_field), ('label', label_field)]
        y = data['Label'].values.tolist()
        for idx, (text, label) in enumerate(zip(X, y)):
            examples.append(Example.fromlist([idx, text, len(text), label], fields))
    return examples, fields



def subset_to_dataset(subset, idx_field, text_field, length_field, label_field):
    fields = [('id', idx_field), ('text', text_field), ('length', length_field), ('label', label_field)]
    examples = []
    for ex in subset:
        examples.append(Example.fromlist([ex.id, ex.text, ex.length, ex.label], fields))
    return examples, fields



def dataProcessing(train, test, fix_length):
    train['Category'] = train['Category'].astype('category')
    category = train['Category'].unique()
    mapping_back = {k:v for k, v in enumerate(category)}
    mapping_to = {v:k for k, v in enumerate(category)}
    train['Label'] = train['Category'].apply(lambda x: mapping_to[x])
    
    
    ### tokenize, stop word, lemmatize
    # tokenize
    tokenizer = lambda x: word_tokenize(x)
    
    # create field
    idx_field = Field(sequential=False, use_vocab=False)
    text_field = Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=fix_length, batch_first=True)
    label_field = LabelField(sequential=False, use_vocab=False)
    length_field = Field(sequential=False, use_vocab=False)
    
    # combine Title and Description
    train["Full_Text"] = train['Title'] + "." + train["Description"]
    test["Full_Text"] = test['Title'] + "." + test["Description"]
    train['Text'] = train['Full_Text'].apply(lambda x: text_field.preprocess(x))
    test['Text'] = test['Full_Text'].apply(lambda x: text_field.preprocess(x))
    
    # stop word
    stopWord_eng = set(stopwords.words('english'))
    train['Text'] = train['Text'].apply(lambda x: [i for i in x if i not in stopWord_eng])
    test['Text'] = test['Text'].apply(lambda x: [i for i in x if i not in stopWord_eng])

    # puntuation
    train['Text'] = train['Text'].apply(lambda x: [i for i in x if i not in string.punctuation])
    test['Text'] = test['Text'].apply(lambda x: [i for i in x if i not in string.punctuation])

    # delete special_text
    del_special_text = ['lt','gt','reuters']
    train['Text'] = train['Text'].apply(lambda x: [i for i in x if i.isalpha() and i not in del_special_text])
    test['Text'] = test['Text'].apply(lambda x: [i for i in x if i.isalpha()and i not in del_special_text])

    # stemmer
    stemmer = PorterStemmer()
    # train['Text'] = train['Text'].apply(lambda x: [stemmer.stem(i) for i in x])
    # test['Text'] = test['Text'].apply(lambda x: [stemmer.stem(i) for i in x])
    
    # lemmatize
    lemmatizer = WordNetLemmatizer()
    # train['Text'] = train['Text'].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])
    # test['Text'] = test['Text'].apply(lambda x: [lemmatizer.lemmatize(i) for i in x])
    
    train['Length'] = train['Text'].apply(lambda x: len(x))
    
    ### train/test data to tensor.Iterator
    train_examples, train_fields = dataframe_to_dataset(train, 'train', idx_field, text_field, length_field, label_field)
    test_examples, test_fields = dataframe_to_dataset(test, 'test', idx_field, text_field, length_field, label_field)
    
    train_dataset = Dataset(train_examples, train_fields)
    test_dataset = Dataset(test_examples, test_fields)
    
    train_iter = BucketIterator(train_dataset, 
                                batch_size=batch_size,
                                sort_key=lambda x: len(x.text),
                                sort=False, 
                                sort_within_batch=True,
                                repeat=False,
                                device=device)
        
    ########  another method is to split a valid set  ########
    ## split valid set
    # train_size = int(len(train) * 0.9)
    # valid_size = len(train) - train_size
    # train_subset, valid_subset = random_split(train_dataset, [train_size, valid_size])
    # train_set, valid_set = train_dataset.split(split_ratio=0.8, random_state=random.seed(100))
    # train_examples_sub, train_fields_sub = subset_to_dataset(train_subset, idx_field, text_field, length_field, label_field)
    # train_set = Dataset(train_examples_sub, train_fields_sub)
    # valid_examples_sub, valid_fields_sub = subset_to_dataset(valid_subset, idx_field, text_field, length_field, label_field)
    # valid_set = Dataset(valid_examples_sub, valid_fields_sub)

    # train_iter, valid_iter = BucketIterator.splits((train_set, valid_set),
    #                                                 batch_size=batch_size,
    #                                                 sort_key=lambda x: len(x.text),
    #                                                 sort=False, 
    #                                                 sort_within_batch=True,
    #                                                 repeat=False,
    #                                                 device=device)


    test_iter = Iterator(test_dataset, 
                         batch_size=400, 
                         sort=False, 
                         sort_within_batch=False, 
                         shuffle=False,
                         repeat=False,
                         device=device)

    ### build train vocab
    text_field.build_vocab(train['Text'], vectors=GloVe(name='6B', dim=embedding_size), min_freq=1)
    
    ## use train and test word to build vocab
    total_text = pd.concat([train['Text'], test['Text']], join='inner')
    total_text = total_text.reset_index()
    text_field.build_vocab(total_text['Text'], vectors=GloVe(name='6B', dim=embedding_size), min_freq=1)

    vocab = text_field.vocab
    
    return train_iter, test_iter, vocab, mapping_back


# See vocab data
# train = pd.read_csv('/content/train.csv')
# test = pd.read_csv('/content/test.csv')

# fix_length = 200
# LR = 0.0008
# epoch = 10
# embedding_size = 300   ### must be divisible by num_heads
# batch_size = 200
# train_iter, test_iter, vocab, mapping_back = dataProcessing(train, test, fix_length)

# print(dir(vocab))
# print(vocab.itos)
# print(vocab.freqs)
# print(vocab.stoi)

"""## Optimizer: ASAM"""

import torch
from collections import defaultdict

class ASAM:
    def __init__(self, optimizer, model, rho=0.5, eta=0.01):
        self.optimizer = optimizer
        self.model = model
        self.rho = rho
        self.eta = eta
        self.state = defaultdict(dict)

    @torch.no_grad()
    def ascent_step(self):
        wgrads = []
        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            t_w = self.state[p].get("eps")
            if t_w is None:
                t_w = torch.clone(p).detach()
                self.state[p]["eps"] = t_w
            if 'weight' in n:
                t_w[...] = p[...]
                t_w.abs_().add_(self.eta)
                p.grad.mul_(t_w)
            wgrads.append(torch.norm(p.grad, p=2))
        wgrad_norm = torch.norm(torch.stack(wgrads), p=2) + 1.e-16
        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            t_w = self.state[p].get("eps")
            if 'weight' in n:
                p.grad.mul_(t_w)
            eps = t_w
            eps[...] = p.grad[...]
            eps.mul_(self.rho / wgrad_norm)
            p.add_(eps)
        self.optimizer.zero_grad()

    @torch.no_grad()
    def descent_step(self):
        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            p.sub_(self.state[p]["eps"])
        self.optimizer.step()
        self.optimizer.zero_grad()


class SAM(ASAM):
    @torch.no_grad()
    def ascent_step(self):
        grads = []
        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            grads.append(torch.norm(p.grad, p=2))
        grad_norm = torch.norm(torch.stack(grads), p=2) + 1.e-16
        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            eps = self.state[p].get("eps")
            if eps is None:
                eps = torch.clone(p).detach()
                self.state[p]["eps"] = eps
            eps[...] = p.grad[...]
            eps.mul_(self.rho / grad_norm)
            p.add_(eps)
        self.optimizer.zero_grad()

"""## Training"""

def fit_model(train_iter, test_iter, mapping_back, vocab, embedding_size, fix_length, epoch=10, batch_size=32, LR=0.0005, num_class=4, num_layers=2, head=10, d_model=128, dropout=0.2):
    Transformer_model = Transformer(len(vocab.vectors), embedding_size, fix_length, d_model, num_layers, head, num_class, vocab.vectors, dropout)
    # print(Transformer_model)
    Transformer_model.to(device)

    # SAMSGD https://github.com/moskomule/sam.pytorch
    # optimizer = SAMSGD(Transformer_model.parameters(), lr=LR, rho=0.05)
    # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6, verbose=True)

    # ASAM+RAdam
    optimizer = optim.RAdam(Transformer_model.parameters(), lr=LR, weight_decay=5e-4)
    minimizer = ASAM(optimizer, Transformer_model, rho=0.05)
    # scheduler = ReduceLROnPlateau(minimizer.optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6, verbose=True)
    criterion = nn.CrossEntropyLoss()

    for ep in range(epoch):
        ### train
        print('epoch: ', ep+1)
        train_correct = 0.0
        train_total = 0.0
        train_error = 0.0
        Transformer_model.train()
        for train_idx, train_batch in enumerate(train_iter):
            optimizer.zero_grad()
            train_X = train_batch.text
            train_y = train_batch.label
            train_L = train_batch.length
            train_mask = Transformer_model.generate_mask(train_L)
            train_pred = Transformer_model(train_X, train_mask, 'train')
            train_loss = criterion(train_pred.view(-1, num_class), train_y)
            train_loss.backward()
            
            # ASAM
            # Ascent Step
            minimizer.ascent_step()
            # Descent Step
            tmp_train_mask = Transformer_model.generate_mask(train_L)
            tmp_train_pred = Transformer_model(train_X, tmp_train_mask, 'train')
            tmp_train_loss = criterion(tmp_train_pred.view(-1, num_class), train_y)
            tmp_train_loss.backward()
            minimizer.descent_step()

            # SAM
            # def closure():
            #   optimizer.zero_grad()
            #   train_mask = Transformer_model.generate_mask(train_L)
            #   output = Transformer_model(train_X, train_mask)
            #   loss = criterion(output.view(-1, num_class), train_y)
            #   loss.backward()
            #   return loss

            # train_loss = optimizer.step(closure)
            # optimizer.step()
            
            _, train_pred = torch.max(train_pred.data, dim=1)
            train_total += train_y.size(0)
            train_correct += (train_pred == train_y).sum().item()
            train_error += train_loss.item()
        
        train_accuracy = 100 * train_correct / train_total
        train_losses = train_error / len(train_iter)
        print('train_accuracy: {} %'.format(train_accuracy))
        print('train_loss: ', train_losses)

        # scheduler.step(train_losses)
        
        ### valid
        # Transformer_model.eval()
        # valid_correct = 0.0
        # valid_total = 0.0
        # valid_error = 0.0
        # with torch.no_grad():
        #     for valid_idx, valid_batch in enumerate(valid_iter):
        #         valid_X = valid_batch.text
        #         valid_y = valid_batch.label
        #         valid_L = valid_batch.length
        #         valid_mask = Transformer_model.generate_mask(valid_L)
            
        #         valid_pred = Transformer_model(valid_X, valid_mask)
        #         valid_loss = criterion(valid_pred, valid_y)
                
        #         _, valid_pred = torch.max(valid_pred.data, dim=1)
        #         valid_total += valid_y.size(0)
        #         valid_correct += (valid_pred == valid_y).sum().item()
        #         valid_error += valid_loss.item()
        #     valid_accuracy = 100 * valid_correct / valid_total
        #     valid_losses = valid_error / len(valid_iter)
        #     print('valid_accuracy: {} %'.format(valid_accuracy))
        #     print('valid_loss: ', valid_losses)


    ### output predict
    Transformer_model.eval()
    predict = []
    with torch.no_grad():
        for test_idx, test_batch in enumerate(test_iter):
            
            test_X = test_batch.text
            test_L = test_batch.length
            test_mask = Transformer_model.generate_mask(test_L)
            
            test_pred = Transformer_model(test_X, test_mask, 'test')
            
            _, test_pred = torch.max(test_pred.data, dim=1)
            predict.extend(test_pred.tolist())

    pred_data = pd.DataFrame({'Id': range(len(predict)), 'Category': predict})
    pred_data['Id'] = np.array(pred_data.index)+1
    pred_data['Category'] = pred_data['Category'].apply(lambda x: mapping_back[x])

    pred_data.to_csv('./310706034_submission_transformer.csv', index=False)

fix_length = 200
LR = 0.0008
epoch = 20
embedding_size = 300   ### must be divisible by num_heads
batch_size = 200

num_layers = 5
head = 15
d_model = 512
num_class = 4
dropout = 0.025


train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

train_iter, test_iter, vocab, mapping_back = dataProcessing(train, test, fix_length)
fit_model(train_iter, test_iter, mapping_back, vocab, embedding_size, fix_length, epoch, batch_size, LR, num_class, num_layers, head, d_model, dropout)

"""## Attention Map"""

import numpy as np 
import seaborn as sns

sentence_id = 9

print('test sentence_id = ', sentence_id)
LABEL = ['World', 'Sports', 'Business', 'Sci/Tech']
pred = pd.read_csv('/content/310706034_submission_transformer.csv')

for test_idx, test_batch in enumerate(test_iter):
    test_X = test_batch.text
    test_L = test_batch.length
    text = test_X.tolist()
    text_length = test_L.tolist()
    score = collection[test_idx]
    sentence_id = sentence_id - 1
    check = list(map(lambda x:vocab.itos[x] ,text[sentence_id][:text_length[sentence_id]]))
    print(check)
    print('predict_label =' ,LABEL[pred['Category'][sentence_id]-1])
    print('sentence length = ', text_length[sentence_id])
    test = np.asarray(collection[test_idx][sentence_id][:text_length[sentence_id]][:,:text_length[sentence_id]])
    x_axis_labels = check # labels for x-axis
    y_axis_labels = check # labels for y-axis
    sns.heatmap(test, xticklabels=x_axis_labels, yticklabels=y_axis_labels)

