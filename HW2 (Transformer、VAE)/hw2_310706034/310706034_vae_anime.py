# -*- coding: utf-8 -*-
"""dl 2_2  animation code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13_8coONMedSJ1ogJWg0QFxNdknMhkJdR
"""

import os
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt # plt 用于显示图片
import matplotlib.image as mpimg # mpimg 用于读取图片
from torchvision.utils import save_image
from torchvision import datasets
from torchvision import transforms

# torch.autograd.set_detect_anomaly(True)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

! unzip anime_faces.zip

class VAE(nn.Module):
    def __init__(self, input_size, hidden_size, feature_size):
        super(VAE, self).__init__()
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        # encoder
        self.encoder1 = nn.Linear(input_size, hidden_size)
        self.encoder2 = nn.Linear(hidden_size, feature_size * 2)
        # decoder
        self.decoder1 = nn.Linear(feature_size, hidden_size)
        self.decoder2 = nn.Linear(hidden_size, input_size)
        
    def encoder(self, x):
        hidden = self.encoder2(self.relu(self.encoder1(x)))
        return hidden.view(-1, 2, feature_size)
    
    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5*log_var)
        epsilon = torch.randn_like(std)
        sample = mu + (epsilon * std)
        return sample
    
    def decoder(self, x):
        hidden = self.sigmoid(self.decoder2(self.relu(self.decoder1(x))))
        return hidden
    
    def cal_loss(self, loss, mu, logvar):
        BCE = loss 
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return BCE, KLD
        
    def forward(self, x):
        out = self.encoder(x)
        mu = out[:, 0, :]
        log_var = out[:, 1, :]
        z = self.reparameterize(mu, log_var)
        reconst = self.decoder(z).type(torch.float32)
        return reconst, mu, log_var, z
    
    def plot_figure_loss(self, loss):
        plt.figure(figsize=(6.4, 4.8))
        plt.plot(loss, label='Training')
        plt.xlabel('Epochs')
        plt.ylabel('Nagetive ELBO')
        
        plt.show()
        


def toDataLoader(data, batch_size):
    image_size = data.shape[2]
    image_type = 3
    dataReshape = data.reshape((data.shape[0], data.shape[1]*data.shape[2]*data.shape[3]))
    input_size = dataReshape.shape[1]
    train_image = torch.tensor(dataReshape)
    train_loader = torch.utils.data.DataLoader(train_image, batch_size=batch_size, shuffle=True)
    return train_loader, image_size, image_type, input_size

def plot_synthesized(model, train_loader, image_type, image_size, scale=None, n=11):
    
    rand = torch.randint(0, len(train_loader.dataset), (2, )).to(device)
    rand_x = train_loader.dataset[rand[0]].type(torch.float32).to(device)
    rand_y = train_loader.dataset[rand[1]].type(torch.float32).to(device)
    
    grid_x = np.linspace(0.0, 1.0, n)
    _, _, _, x_z = model(rand_x)
    _, _, _, y_z = model(rand_y)
    synTensor = torch.zeros((n, image_type, image_size, image_size))
    for i, xi in enumerate(grid_x):
        z_sample = x_z*xi + y_z*(1-xi)
        reconstr_syn = model.decoder(z_sample).view(-1, image_type, image_size, image_size)    # (n, 3, 64, 64)
        synTensor[i, :, :, :] = reconstr_syn
    if scale == None:
        save_image(synTensor.view(-1, image_type, image_size, image_size), 'anime_synthesized.png', nrow=n)
    else:
        save_image(synTensor.view(-1, image_type, image_size, image_size), 'anime_synthesized_{}.png'.format(scale), nrow=n)


def fit_model(train_loader, image_size, image_type, input_size, epoch=10, batch_size=64, LR=0.001, scale=None):
    VAE_model = VAE(input_size, hidden_size, feature_size).to(device)
    optimizer = optim.Adam(VAE_model.parameters(), lr=LR)
    criterion = nn.BCELoss(reduction='sum')
    
    train_lossList = []
    for ep in range(epoch):
        print('epoch: ', ep+1)
        train_error = 0.0
        ### train
        VAE_model.train()
        for train_X in train_loader:
            train_X = train_X.type(torch.float32).to(device)
            if train_X.size(0) == 64:
                origin_image = train_X
            
            optimizer.zero_grad()
            train_reconst, train_mu, train_logvar, train_z = VAE_model(train_X)
            train_bceLoss = criterion(train_reconst, train_X)
            train_BCE, train_KLD = VAE_model.cal_loss(train_bceLoss, train_mu, train_logvar)
            if scale == None:
                train_loss = train_BCE + train_KLD
            else:
                train_loss = train_BCE + scale / 100 * train_KLD
            train_loss = train_BCE + train_KLD
            train_loss.backward()
            optimizer.step()
            train_error += train_loss.item()
        
        train_losses = train_error / len(train_loader.dataset)
        print('train_loss: ', train_losses)
        train_lossList.append(train_losses)
        
        ### 輸出每回 epoch 最後一個 batch_size=64 的圖
        VAE_model.eval()
        with torch.no_grad():
            const_image, _, _, _ = VAE_model(train_X)
            # (B x C x H x W)
            concat_image = torch.cat([train_X.view(-1, image_type, image_size, image_size), const_image.view(-1, image_type, image_size, image_size)], dim=3)
            if scale == None:
                save_image(concat_image, 'anime_{}.png'.format(ep))
            else:
                save_image(concat_image, 'anime_{}_{}.png'.format(ep, scale))
            
    
    ### plot loss figure
    VAE_model.plot_figure_loss(train_lossList)
    
    ### output the reconstruction/sample image
    VAE_model.eval()
    with torch.no_grad():
        const_image, _, _, _ = VAE_model(origin_image)
        if scale == None:
            save_image(origin_image.view(-1, image_type, image_size, image_size), 'anime_origin.png')
            save_image(const_image.view(-1, image_type, image_size, image_size), 'anime_reconst.png')
        else:
            save_image(origin_image.view(-1, image_type, image_size, image_size), 'anime_origin_{}.png'.format(scale))
            save_image(const_image.view(-1, image_type, image_size, image_size), 'anime_reconst_{}.png'.format(scale))
        
        sample = torch.randn(batch_size, feature_size).to(device)
        sample_image = VAE_model.decoder(sample).view(-1, image_type, image_size, image_size)
        if scale == None:
            save_image(sample_image.view(-1, image_type, image_size, image_size), 'anime_sample.png')
        else:
            save_image(sample_image.view(-1, image_type, image_size, image_size), 'anime_sample_{}.png'.format(scale))
    
    ### save the synthesized image
    plot_synthesized(VAE_model, train_loader, image_type, image_size, scale)


#%%

epoch = 10
batch_size = 64
LR = 0.001

hidden_size = 512
feature_size = 16


animeList = []
for i in range(1, 10000+1):
    img = cv2.imread('/content/anime_faces/'+str(i)+'.png')
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    animeList.append(img.transpose(2, 0, 1) / 255)
anime = np.array(animeList)

train_loader, image_size, image_type, input_size = train_loader, image_size, image_type, input_size = toDataLoader(anime, batch_size)

fit_model(train_loader, image_size, image_type, input_size, epoch, batch_size, LR)

# scaleList = [i for i in range(0, 100, 33)]
for scale in [25, 75]:
    print('scale: ', scale)
    fit_model(train_loader, image_size, image_type, input_size, epoch, batch_size, LR, scale)

